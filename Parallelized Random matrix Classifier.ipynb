{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "disabled-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from numba import njit,prange,jit,guvectorize\n",
    "import numba\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV,KFold,cross_val_score,train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,plot_confusion_matrix, auc as aucc,plot_roc_curve,recall_score,precision_score,f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import os\n",
    "from scipy.special import expit,softmax\n",
    "import tensorflow as tf\n",
    "from scipy.special import expit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "peripheral-thickness",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-76c65d93dfc8>:50: NumbaPerformanceWarning: \u001b[1m\u001b[1mnp.vdot() is faster on contiguous arrays, called on (array(complex128, 1d, A), array(complex128, 1d, A))\u001b[0m\u001b[0m\n",
      "  dotprod= np.vdot(feature_vecs[idx],trial_mol).real\n"
     ]
    }
   ],
   "source": [
    "#The main class defining the classifier\n",
    "class RMTClassifier(object):\n",
    "    def __init__(self,threshold=1,train_cutoff=0.92):\n",
    "        self.threshold=threshold\n",
    "        self.train_cutoff=train_cutoff\n",
    "        self.epsilon = None\n",
    "        self.feature_vecs = None\n",
    "        self.dim_v= None\n",
    "        self.p = None\n",
    "    def fit(self,train_mat):\n",
    "        n,p=train_mat.shape\n",
    "        self.p=p\n",
    "        gamma= p/n\n",
    "        thresh= ((1+np.sqrt(gamma))**2)*self.threshold\n",
    "        c_mat=np.dot(train_mat.T,train_mat)/n\n",
    "\n",
    "        evals,evecs=np.linalg.eig(c_mat)\n",
    "        idx= evals.argsort()\n",
    "        idx= idx[::-1]\n",
    "        evals,evecs= evals[idx],evecs[:,idx]\n",
    "        \n",
    "\n",
    "        dim_v= evals[evals>thresh].shape[0]\n",
    "        feature_vecs= np.ascontiguousarray(evecs[:,:dim_v].T,dtype=np.complex128)\n",
    "        self.dim_v,self.feature_vecs=dim_v,feature_vecs\n",
    "        train_mat=np.ascontiguousarray(train_mat,dtype=np.complex128)\n",
    "        \n",
    "        trial_mol_proj= np.zeros((self.dim_v,self.p),dtype=np.complex128)\n",
    "        similarity=find_similarity2(train_mat,trial_mol_proj,feature_vecs)\n",
    "        similarity.sort()\n",
    "  \n",
    "        cutoff_idx= int(self.train_cutoff*len(similarity))\n",
    "        epsilon= similarity[cutoff_idx]\n",
    "        self.epsilon= epsilon\n",
    "\n",
    "    def predict(self,test, epsilon_multiple = 1):\n",
    "        test=np.ascontiguousarray(test,dtype=np.complex128)\n",
    "        trial_mol_proj= np.zeros((self.dim_v,self.p),dtype=np.complex128)\n",
    "        test_similarity=find_similarity2(test,trial_mol_proj,self.feature_vecs)\n",
    "        \n",
    "        #predictions= np.array([1 if x<self.epsilon*epsilon_multiple else 0 for x in test_similarity])\n",
    "        predictions= test_similarity,self.epsilon\n",
    "        return predictions\n",
    "\n",
    "\n",
    "@guvectorize([\"c16[:],c16[:,:],c16[:,:],f8[:]\"], \"(p),(dim_v,p), (dim_v,p) -> ()\", nopython=True, target=\"parallel\")\n",
    "def find_similarity2(trial_mol,trial_mol_proj,feature_vecs,similarity):\n",
    "    dummy=np.zeros_like(trial_mol_proj)\n",
    "    for idx in prange(feature_vecs.shape[0]):\n",
    "        dotprod= np.vdot(feature_vecs[idx],trial_mol).real\n",
    "        dummy[idx]=dotprod*feature_vecs[idx]  \n",
    "    projection=np.sum(dummy,axis=0).real\n",
    "    similarity[0]=np.linalg.norm(trial_mol-projection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "friendly-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns one classifier for the active samples and the other for incative samples\n",
    "def fit_RMT(x_act,x_inact):\n",
    "    clf_act=RMTClassifier()\n",
    "    clf_inact=RMTClassifier()\n",
    "    clf_act.fit(x_act)\n",
    "    clf_inact.fit(x_inact)\n",
    "    return clf_act,clf_inact\n",
    "\n",
    "#This function uses both the classifiers to predict the scores of the sample being in active class\n",
    "def clf_predict(clf_act,clf_inact,x_test,e=1,thresh=True,prob=False):\n",
    "    pred1,epsilon1= clf_act.predict(x_test)\n",
    "    pred0,epsilon0= clf_inact.predict(x_test)\n",
    "    diff=expit(pred1-pred0)\n",
    "    epsilon= expit(epsilon1-epsilon0)\n",
    "    if prob==True: return diff\n",
    "    else:    \n",
    "        if thresh==False: pred=[1 if i<e else 0 for i in diff]\n",
    "        else: pred=[1 if i<epsilon*e else 0 for i in diff]\n",
    "        return pred\n",
    "\n",
    "    \n",
    "    \n",
    "#Some random stuff to keep a check on different metrics\n",
    "def FP_TP_FN_TN(y_test,y_pred):\n",
    "    y1,y2=y_test.ravel(),y_pred.ravel()\n",
    "    fp,tp,fn,tn=0,0,0,0\n",
    "    for i in prange(len(y1)):\n",
    "        a,b=y1[i],y2[i]\n",
    "        if(a==1): \n",
    "            if(b==1):tp+=1\n",
    "            else:fn+=1\n",
    "        else:\n",
    "            if(b==1):fp+=1\n",
    "            else:tn+=1\n",
    "    \n",
    "    return fp,tp,fn,tn\n",
    "\n",
    "def optimized_thresh(clf_act,clf_inact,x_train,y_train,y):\n",
    "\n",
    "    prob1,e1=clf_act.predict(x_train)\n",
    "    prob0,e0=clf_inact.predict(x_train)\n",
    "    diff=prob1-prob0\n",
    "    e_diff=e1-e0\n",
    "    e,max_acc=0,0\n",
    "    for i in np.linspace(e_diff-5,e_diff+5,100):\n",
    "        acc=np.mean((diff<i)==(y_train==y))\n",
    "        if acc>max_acc:\n",
    "            max_acc=acc\n",
    "            e=i\n",
    "    return e,max_acc\n",
    "\n",
    "def metric_RMT(clf1,clf0,x_test,y_test,plot=True):\n",
    "    x,y,p,r,f1,e=[],[],[],[],[],[]\n",
    "    best_f1=0\n",
    "    arr[0]=0\n",
    "    prob=clf_predict(clf1,clf0,x_test,thresh=False,prob=True)\n",
    "    for idx,val in enumerate(arr):\n",
    "      #if idx%5==0:\n",
    "          #print(f'progress:{idx}%')\n",
    "        y_pred=np.where(prob<val,1,0)\n",
    "        fp,tp,fn,tn= FP_TP_FN_TN(np.array(y_test),np.array(y_pred))\n",
    "        tpr=tp/(tp+fn)\n",
    "        fpr=fp/(fp+tn)\n",
    "        if (tp+fp)==0 : precision=1\n",
    "        else: precision= tp/(tp+fp)\n",
    "        recall= tpr\n",
    "        if precision+recall==0: f1_sc=0\n",
    "        else:f1_sc=(2*precision*recall)/(precision+recall)\n",
    "        f1.append(f1_sc)\n",
    "        p.append(precision)\n",
    "        r.append(recall)\n",
    "        e.append(val)\n",
    "        x.append(fpr)\n",
    "        y.append(tpr)\n",
    "        print('e=',val,'f1=',f1_sc)\n",
    "        print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "        if (best_f1<f1_sc):\n",
    "            best_f1,i=f1_sc,idx  \n",
    " \n",
    "    AUC_ROC,AUC_PR=aucc(x,y),aucc(r[1:],p[1:])\n",
    "    if(plot==True):\n",
    "    \n",
    "        fig1,fig2=plt.figure(),plt.figure()\n",
    "        ax1,ax2=fig1.add_subplot(111),fig2.add_subplot(111)\n",
    "        ax1.set_title('ROC')\n",
    "        ax1.set_ylabel('True Positive')\n",
    "        ax1.set_xlabel('False Positive')\n",
    "        ax1.text(0.75,0.15,f'AUC:{str(np.round(AUC_ROC,4))}')\n",
    "        ax1.plot(x,y)\n",
    "        ax1.axis([0,1,0,1])\n",
    "        ax2.set_title('Precision/Recall')\n",
    "        ax2.set_ylabel('Precision')\n",
    "        ax2.set_xlabel('Recall')\n",
    "        ax2.text(0.75,0.15,f'AUC-PR:{str(np.round(AUC_PR,4))}')\n",
    "        ax2.plot(r,p)\n",
    "        ax2.axis([0,1,0,1])\n",
    "        plt.show()\n",
    "  \n",
    "    return AUC_ROC,AUC_PR,p,r,f1,best_f1,e,AUC_ROC*AUC_PR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-afghanistan",
   "metadata": {},
   "source": [
    "# Example application on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "included-darwin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "#Loading and preprocessing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "classes=[0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "def preprocess_and_arrange(x_data,y_data):\n",
    "    x_data=(x_data/127.5)-1\n",
    "    x_data=np.reshape(x_data,(len(x_data),28*28))\n",
    "    idx=y_data.argsort()\n",
    "    y_data=y_data[idx]\n",
    "    x_data=x_data[idx]\n",
    "\n",
    "    return x_data,y_data\n",
    "    \n",
    "x_train,y_train=preprocess_and_arrange(x_train,y_train)\n",
    "x_test,y_test=preprocess_and_arrange(x_test,y_test)\n",
    "\n",
    "#index list for all classes\n",
    "idx=[]                            \n",
    "c='a'\n",
    "for i in np.arange(len(y_train)):\n",
    "    if c!=y_train[i]:\n",
    "        idx.append(i)\n",
    "        c=y_train[i]\n",
    "idx.append(len(y_train))\n",
    "print(x_train.shape,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "passive-former",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.79111814498901 s\n"
     ]
    }
   ],
   "source": [
    "#train and optimize classifiers for each class, one to find whether the sample is form class x and the other for not in class x, total of 20 classifiers\n",
    "classy1,classy0,es,individual_accs=[],[],[],[]\n",
    "for i in np.arange(len(classes)):\n",
    "    y=classes[i]\n",
    "    act=x_train[idx[i]:idx[i+1]]\n",
    "    inact=np.concatenate((x_train[0:idx[i]],x_train[idx[i+1]:]))\n",
    "    clf_act,clf_inact=fit_RMT(act,inact)\n",
    "    e,max_acc=optimized_thresh(clf_act,clf_inact,x_train,y_train,y)\n",
    "    classy1.append(clf_act),classy0.append(clf_inact),es.append(e),individual_accs.append(max_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "resident-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the probabilities of each classifier for each class\n",
    "def evaluate(x_test,classy0,classy1,es):\n",
    "    probs=np.zeros((len(x_test),len(classy0)))\n",
    "    for i in np.arange(len(classy0)):\n",
    "        clf_act=classy1[i]\n",
    "        clf_inact=classy0[i]\n",
    "        prob1,e1=clf_act.predict(x_test)\n",
    "        prob0,e0=clf_inact.predict(x_test)\n",
    "        diff=prob1-prob0-es[i]\n",
    "        diff=(diff-np.mean(diff))/np.std(diff)\n",
    "        probs[:,i]=diff\n",
    "    \n",
    "    probs=softmax(-probs,axis=1)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sunrise-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=evaluate(x_test,classy0,classy1,es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "immune-titanium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test =  0.952\n"
     ]
    }
   ],
   "source": [
    "accuracy=np.mean(np.argmax(probs,axis=1)==y_test)\n",
    "print(\"Accuracy on test = \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-explosion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
